---
title: "Wallace Vignette"
author:
- Jamie M. Kass
- Sarah S. Meenan
- Gonzalo Pinilla
- Cory Merow
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Wallace Vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, echo = FALSE, include=FALSE}
library(knitr)
knitr::opts_chunk$set(collapse=TRUE, message=FALSE, warning=FALSE, comment="#>")
```

# Preface
This vignette was written for *Wallace* v. 1.0.4, so if you are using a more updated version, some things will be different. Additionally, this vignette and any others in the `wallace` package will be updated regularly in accordance with ongoing development. 

# Introduction
*Wallace* is an `R`-based GUI application for ecological modeling that currently focuses on building, evaluating, and visualizing models of species niches and distributions. We will refer to these models as species distribution models (SDMs), and we will not explain them at length here---as you read through, you will be pointed to some sources of detailed info within the application for reference.

*Wallace* has many qualities which we think make it a good example of next-generation scientific software: it's 1) open, 2) expandable, 3) flexible, 4) interactive, 5) instructive, and 6) reproducible. The application features an pannable/zoomable map and dynamic plots and tables. Data for the models can be download from online databases or uploaded by the user, and most results can be downloaded, including the option to save R code that can reproduce your analysis. For more details, including on SDMs, please see our [publication](http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12945/full) in *Methods in Ecology and Evolution*. The citation is below:

Kass JM, Vilela B, Aiello-Lammens ME, Muscarella R, Merow C, Anderson RP. Wallace: A flexible platform for reproducible modeling of species niches and distributions built for community expansion. *Methods Ecol Evol*. 2018;00:1–6. https://doi.org/10.1111/2041-210X.12945

The *Wallace* project's [main page](https://wallaceecomod.github.io/) has links to the [Google Group](https://groups.google.com/forum/#!forum/wallaceecomod), the official [email](mailto:wallaceecomod@gmail.com), the [CRAN page](https://CRAN.R-project.org/package=wallace) hosting the stable version, and the [Github development page](https://github.com/wallaceEcoMod/wallace).

# Setup
For `wallace` to work, **you should be using the latest version of R** (or at least later than version 3.2.1). Download for
[Windows](https://cran.r-project.org/bin/windows/base/) or [Mac](https://cran.r-project.org/bin/macosx/).

Let's first install and run *Wallace*.
```{r, eval=FALSE}
# install the package
install.packages('wallace')
# load the package
library(wallace)
# run the app
run_wallace()
```

The *Wallace* GUI will open in your default web browser and the `R` console will be occupied (you can exit *Wallace* by pushing `Escape`). A note: if you close the browser window running *Wallace*, **your session will be over and all progress will be lost**. 

```{r, out.width = "600px", echo = FALSE}
knitr::include_graphics("vignette_img/Wallace_Intro.png")
```

If you'd like to use the `R` console while running *Wallace*, open a terminal window (MacOS/Linux) or command prompt (Windows), initialize `R`, and then run the lines above. An example with Terminal in MacOS is below.

```{r, out.width = "800px", echo = FALSE}
knitr::include_graphics("vignette_img/Wallace_Introb.png")
```

# Obtain Occurrence Data

You will notice tabs along the top: these are the “components”, which represent discrete steps of the analysis, and you will be stepping sequentially through them. The first step is to obtain occurrence data for our analysis, so we will click on the “Occ Data” component tab. On the left side, there is a window where all the user interface controls are (buttons, text inputs, etc.). You can see that the “module” called “Query Database” is currently selected. “Modules” are discrete analysis options within each component, and can be contributed by other researchers. You'll see that another module exists for this component: “User-specified Occurrences”. This module lets you upload your own occurrence data. Try choosing this module instead and notice that the controls change, then click back to “Query Database”.

```{r, out.width = "800px", echo = FALSE}
knitr::include_graphics("vignette_img/Wallace1a.png")
```

On the right side is the interaction window, which has tabs representing the map, occurrence records table, results window, and guidance text windows for both the component and module levels. At this stage of the analysis, no results exist, and we have no data yet for the table, but you can view the guidance text now. This text was written by the developers to prepare users for each component and module *methodologically* (what the tools do) and *theoretically* (why we should use them). The guidance text also references scientific papers from the ecology literature for more detailed reading. Please get into the habit of consulting these before undertaking analyses, as they should give you a more solid foundation for moving forward.

```{r, out.width = "800px", echo = FALSE}
knitr::include_graphics("vignette_img/Wallace1b.png")
```

Begin by downloading occurrence records for *Tremarctos ornatus* (spectacled bear) from GBIF. Set the maximum number of occurrences to 200 and click the **Query Database** button. After the download is complete, notice the message in the log window, and click on the table tab to view more information on the records. The developers chose the fields that are displayed based on their general relevance to SDMs. The black arrow shows the button you can press to **Download** a .csv file of these records, which has all the original database fields.

```{r, out.width = "800px", echo = FALSE}
knitr::include_graphics("vignette_img/Wallace1b.png")
```

At this stage, it is a good idea to read the component and module guidance text by clicking on their tabs.

```{r, out.width = "800px", echo = FALSE}
knitr::include_graphics("vignette_img/Wallace1c.png")
```

# Process Occurrence Data

The next component, “Process Occs”, gives you access to some data-cleaning tools. If you want to model the distribution of *T. ornatus* in South America, it makes sense to remove the point found in California, the one in Japan, the one off the coast of Africa, and the one in the antarctic. For databases like GBIF that accumulate lots of data from various sources, there are inevitably always some dubious localities that may represent a museum collection instead of a natural occurrence, or simply have incorrect coordinates. In order to eliminate these obviously erroneous records, select the points you want to use by clicking on the module "Select Occurrences On Map". Click on the polygon icon on the map and draw a polygon around the points found in South and Central America. When you are done, click "finish". Now click **Select Occurrences**.

2 a process occs.png
```{r, out.width = "800px", echo = FALSE}
knitr::include_graphics("vignette_img/Wallace2a.png")
```
2 b process occs.png

You can also remove occurrences by ID. Before building your model, it is a good idea learn about the expected range and habitat preferences of the species you are studying in order to remove dubious localities that may be less obvious. For example, *T. ornatus* is a mountain-dwelling species. By zooming in on the points, you can see that some of the localities are in the lowland and likely have incorrect coordinates. Including points like these could negatively impact the model. To remove these, click the point to see its ID and geographic coordinates, and then enter the ID in the control window on the left to remove it. 

Insert image here
2 e process occs.png

The samples may exhibit spatial autocorrelation, which can lead to bias in the environmental signal. For example, there might be a cluster of records near cities because these are mostly from iNaturalist (citizen science) and most citizen scientists live near cities. You can reduce spatial bias by thinning the points to make sure they're all at least 20 km from one another. That leaves 32 points for modeling (yours may be different).

2process_occs_thin1.png
```{r, out.width = "800px", echo = FALSE}
knitr::include_graphics("vignette_img/Wallace2b.png")
```

If you zoom in on the map, you will see the points that were removed in blue.

Insert image here
2process_occs_thinZoom.png

# Obtain Environmental Data

Next, you will need to obtain climatic variables for your model. Click on the component "Env Data". Worldclim is a global climate database that is very popular because it is easily accessible at fine resolutions. The variables are based on interpolations of weather station data for temperature and precipitation, and the coverage is better for areas with more weather stations (especially in developed countries). The **bioclim** predictors are summaries of temperature and precipitation that have been determined to have some biological significance. They're all listed [here](http://www.worldclim.org/bioclim).

Choose the **10 arcmin** bioclimatic variable resolution and click the **Load Env Data** button. Note that you have the option to specify which variables to use in the analysis. The first time you use *Wallace* these data are downloaded to your hard drive; after that they will simply be loaded from this local directory. Finer resolutions will take longer to download. After the environmental layers have loaded the results should appear, displaying information about the raster stack (e.g. the resolution and extent of the data). 

3 Env Data.png
```{r, out.width = "800px", echo = FALSE}
knitr::include_graphics("vignette_img/Wallace3a.png")
```


# Process Environmental Data

Now you will need to choose the study extent for modeling. This will define the region from which "background" values are drawn for model fitting. Methods like Maxent are known as presence-background techniques because they compare the predictor variable values at background locations to those at the occurrence points. In making this decision, we want to avoid areas the species has historically been unable to move to, for example, regions beyond a barrier like a mountain range or large river that the species cannot traverse. If you include these areas, it will send a false signal to the model that these areas are part of the "background" and thus not suitable -- they might be occupied if the species could move there. Please see the guidance text for more details.

You can explore the different options for delineating the study extent here. To begin, go to the module “Select Study Region”. Under “Step 1”, choose “Minimum convex polygon”. Set the study region buffer distance to 1 degree. Click the **Select** button to trim the environmental layers to this polygon. Now, complete “Step 2” by setting the number of background points to 10,000, and clicking the **Sample** button.

4 Process Env Step 1
```{r, out.width = "800px", echo = FALSE}
knitr::include_graphics("vignette_img/Wallace4a.png")
```

# Partition occurrences

In order to check whether you've built a model with strong predictive ability, you theoretically need independent data to validate it. When no independent datasets exist, one solution is to partition your data into subsets that we assume are independent of each other, then sequentially build a model on all the subsets but one and evaluate this model on the left-out subset. This is known as *k*-fold cross validation (where *k* is the total number of subsets), and it is quite prevalent in statistics, especially the field of machine learning. After this sequential model- building exercise is complete, *Wallace* builds a model using **all** the data and summarizes the statistics for all the partitions.

There's a whole literature on how to best partition data for modeling. One option is to simply partition randomly, but with spatial data we run the risk that the groups are not spatially independent of each other. An arguably better option is to partition using spatial blocking -- for example, by drawing lines on a map to divide the data. Spatial partitioning with *k*-fold cross validation forces the model to predict to regions that are distant from those used to train the model. For *Tremarctos ornatus*, environmental conditions in the Andes of Ecuador and western Colombia may differ considerably from conditions in southern Peru. If the model has accurate predictions on average, it likely has good transferability, as it can transfer well to new values of predictor variables (as distant areas are usually more environmentally different than close areas). Please refer to the guidance text for more details on all the types of partitioning offered in *Wallace*.

Below are some options for spatial and nonspatial partitioning.

5 partition nonspatial.png
```{r, out.width = "800px", echo = FALSE}
knitr::include_graphics("vignette_img/Wallace5a.png")
```

Insert image.
5 partition spatial.png

Take a moment to scroll through the log window at the top of the screen and review all the steps you've taken so far.

# Model

You are now ready to build a Maxent model. Maxent is a machine learning method that can fit complex (i.e. curvy) functions to patterns in the data. If you construct it with particular settings it can also become very similar to a generalized linear model (GLM). Thus, Maxent can cover a wide spectrum of complexity. For more details, please consult the guidance text.

Below, we've chosen some modeling options:

  - Select L, LQ, H, and LQH feature classes. These are the shapes that can be fit to the data:
    + L = Linear, e.g. temp + precip
    + Q = Quadratic, e.g. temp^2 + precip^2
    + H = Hinge, e.g. piecewise linear functions, like splines
    
Hinge features can fit all possible slopes between two data points, enabling a very flexible function, similar to a generalized additive model (GAM).

```{r, out.width = "800px", echo = FALSE}
knitr::include_graphics("vignette_img/hinge_threshold.png")
```

  - Select regularization multipliers between 1-3
    + Regularization is a way to reduce model complexity.
    + Higher values = smoother, less complex models. Basically, all predictor variable coefficients are shrunk progressively until some reach 0, when they drop out of the model. Only those variables with the greatest predictive contribution remain in the model.
  - RM Step Value = .5
    + This represents how large a step is taken between RM values in the slide bar.

The modeling takes about 7 minutes. The 4 feature class combinations (L, LQ, H, LQH) \* 5 regularization multipliers (1, 1.5, 2, 2.5, 3) = 20 models. Further, LQH will enable significant complexity in the response, which takes a bit longer to run than simpler models.

6 model a.png
```{r, out.width = "800px", echo = FALSE}
knitr::include_graphics("vignette_img/Wallace6a.png")
```

Insert image here
6 model b.png


The first time you run this, you may get an error message. You need to put the maxent software (maxent.jar) in the directory where *Wallace* will look for it. This is due to the particular way the R package `dismo` was constructed, and out of our control. We are however working on alternative ways to approach this. As the log window indicates, download the file and put it in the appropriate directory. Then click **Run** again.

The results appear in two tables of evaluation statistics, courtesy of R package `ENMeval`, allowing comparison of the different models you just built. There should be 20 rows per table: one for each of the feature class / regularization multiplier combinations you selected. In the first table, statistics from the models built from the 4 occurrence data partitions are averaged -- these are labeled with "test". In the second table, statistics from each of the 4 bins are displayed separately. 

How do we choose the "best" model? There is a mountain of literature about this, and there is really no concrete answer. AUC and OR (omission rate) were calculated using our spatial partitions, and AIC was instead calculated using the model prediction of our background extent. Although AIC does not incorporate the cross validation results, it does explicitly penalize model complexity -- models with more parameters have a worse AIC score. It's really up to the user to decide, and the guidance text has some references which should help you learn more.


```{r, out.width = "800px", echo = FALSE}
knitr::include_graphics("vignette_img/Wallace6b.png")
```

# Visualize

The module “Maxent Evaluation Plots” enables users to evaluate the performance statistics across models. Note that for all four feature classes, models with the regularization multiplier (RM) 3 have the lowest average omission rate (using 10% training presence threshold.) 

7 visualize.png
```{r, out.width = "800px", echo = FALSE}
knitr::include_graphics("vignette_img/Wallace7a.png")
```

To evaluate whether a model makes biological sense, examine the **response curves** that define how each predictor (x-axis) relates to suitability (y-axis). If you want to see the results for a particular model, you can select it by using the dropdown menu under “current model”. Below is one of the response curve for LQ_1.5, mean temperature of the coldest quarter (bio11). The shape of the response indicates that there is an optimal range for this variable, above or below which suitability for *T. ornatus* decreases.  

7 visualize curve.png
```{r, out.width = "800px", echo = FALSE}
knitr::include_graphics("vignette_img/Wallace7b.png")
```

You can also visualize model predictions on the map. Predictions of suitability can be continuous (no threshold) or binary (minimum training presence threshold or 10 percentile training presence threshold). Please see the module guidance for information about Maxent model output scales and thresholding rules. Below is the mapped prediction for model LQ_3, no threshold. At first glance, it looks like a good model because most of the presence points correspond to regions of higher suitability. 

7 visualize prediction.png
```{r, out.width = "800px", echo = FALSE}
knitr::include_graphics("vignette_img/Wallace7c.png")
```

Below is the mapped prediction of the same model, this time with the threshold set to 10 percentile training presence. This is the stricter of the two thresholding rules. You can see that some of the occurrence records fall outside the blue areas that represent suitable areas for *T. ornatus*. Try mapping the prediction with the threshold set to minimum training presence instead, and see the difference.

Insert image here. 
7 Visualize binary.png

# Project

Next, you can project the model to new locations (extending the domain) and future climate scenarios (for years 2050 and 2070). "Projecting" simply means plugging in new values to the mathematical model and getting a new response. Think of it like drawing some new range on the response curves and finding the resulting suitability. This could possibly be outside the range seen previously, and the lines would need to be extended (unless clamping is chosen -- see guidance text).

This is potentially confusing -- didn't the cross validation step do this too? The cross validation with spatial partitioning sequentially forced models to predict to new areas, and the average testing AUC summarized their ability to transfer accurately. However, the final model was built with **all** the data. We are now taking this model and projecting it to predictor variable value ranges that were potentially never used in model training. Thus, these values for different places and times might be completely new to our model, and could be so different that we may be uncertain in the accuracy of our projection. Please see the guidance text for more details.

LQ_3 has a low OR (so it rarely fails to predict known occurrences) and a high average testing AUC (so it should have good transferability). Below, model LQ_3 has been projected to the year 2070 under a severe climate scenario (representative concentration pathway(RCP) 8.5). Notice that there are several global circulation models (GCMs) to choose from. To project your model, draw a polygon like the one below and click “Finish”. Then choose a year, GCM and RCP and click the **Project** button to build the new map. Not all GCMs have data for all available RCPs. If an error message appears in the log window, you may have to try another combination of GCM and RCP. If you mirror the settings below, you can find out how the model L_3 predicts the suitability for *T. ornatus* will look in 2070 under a severe climate change scenario. Note that the suitable area appears to have contracted to an even narrower strip at higher elevations in the Andes mountain range.

```{r, out.width = "800px", echo = FALSE}
knitr::include_graphics("vignette_img/Wallace8a.png")
```


```{r, out.width = "800px", echo = FALSE}
knitr::include_graphics("vignette_img/Wallace8b.png")
knitr::include_graphics("vignette_img/Wallace8c.png")
knitr::include_graphics("vignette_img/Wallace8d.png")
```

# Extracting the code

A major advantage of *Wallace* compared to other GUI-based software is that you can extract all the code used to run the analysis. This allows you to rerun the analysis session, share it, or modify it. The code can be downloaded in several ways, but the **R Markdown** format, which is a convenient format for combining R code and text, can be run directly in R. For .pdf downloads, some version of TeX is necessary. Please see the text on this page for more details.

To download the code, select Rmd and click Download.

```{r, out.width = "800px", echo = FALSE}
knitr::include_graphics("vignette_img/Wallace9a.png")
```

Now, you should have an .Rmd file that contains your complete analysis. Rmd files combine regular text with **code chunks**, shown by the red arrow below. Modules from *Wallace* are indicated as headers denoted by **###**. For a quick reference to Rmd syntax, see [here](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf)

```{r, out.width = "800px", echo = FALSE}
knitr::include_graphics("vignette_img/WallaceRMDa.png")
```

You might want to open a new R window and try running some of this code. Just note that if you close your *Wallace* session you'll lose your progress in the web browser (but your Rmd will be unaffected). If you use RStudio, you can open this Rmd and click **knit** to compile your workflow into a sharable html document.

Note that you can change anything you like in this code to build upon your workflow. We envision that future versions of *Wallace* will enable you to upload such modified Rmds to *Wallace* to fill in all the options you chose and pick up where you left off in a previous analysis in the GUI.

At the moment we don't have anything built into *Wallace* for post-processing, so you can use R directly to build from the code created above.





